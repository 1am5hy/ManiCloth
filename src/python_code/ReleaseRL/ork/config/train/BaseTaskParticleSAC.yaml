# ========== Trainer parameters ==========
Trainer:
#  task_name: BaseTaskParticle              # Task name for logging.
  experiment_name:                    # Experiment name for logging.
  experiment_directory:               # Experiment directory for logging.
  write_interval: 150                 # TensorBoard write interval for logging. (timesteps)
  checkpoint_interval: 750           # Checkpoint interval for logging. (timesteps)
  wandb: True                                  # If true, log to Weights & Biases.
  wandb_kwargs:                       # Weights & Biases kwargs. https://docs.wandb.ai/ref/python/init
    project: RofuncRL                          # Weights & Biases project name.
    name: ${..experiment_name}                 # Weights & Biases run name.
  rofunc_logger_kwargs:               # Rofunc BeautyLogger kwargs.
    verbose: True                     # If true, print to stdout.
  maximum_steps: 1000000               # The maximum number of steps to run for.
  random_steps: 0                     # The number of random exploration steps to take.
  start_learning_steps: 0             # The number of steps to take before starting network updating.
  seed: 0                            # The random seed.
  rollouts: 750                     # The number of rollouts before updating.
  eval_flag: False                     # If true, run evaluation.
  eval_freq: 1500                     # The frequency of evaluation. (timesteps)
  eval_steps: 1500                    # The number of steps to run for evaluation.
  inference_steps: 150               # The number of steps to run for inference.
  max_episode_steps: 150              # The maximum number of steps per episode.
# ========== Agent parameters ==========
Agent:
  discount: 0.99                      # The discount factor, gamma.
  polyak: 0.005                       # soft update hyperparameter (tau)

  gradient_steps: 1                  # The number of gradient steps to take per update.
  batch_size: 256                     # The number of samples per update.

  lr_a: 3e-4                        # Learning rate for actor.
  lr_c: 3e-4                        # Learning rate for critic.
#  lr_scheduler:                    # Learning rate scheduler type.
#  lr_scheduler_kwargs:             # Learning rate scheduler kwargs.
  adam_eps: 1e-5                    # Adam epsilon.

  learn_entropy: True              # If true, learn entropy coefficient.
  entropy_learning_rate: 3e-4      # Entropy learning rate.
  initial_entropy_value: 0.2       # initial entropy value
  target_entropy:                  # target entropy

  grad_norm_clip: 0.0              # clipping coefficient for the norm of the gradients
  kl_threshold: 0                  # Initial coefficient for KL divergence.

#  state_preprocessor:            # State preprocessor type.
#  state_preprocessor_kwargs:     # State preprocessor kwargs.
#  value_preprocessor:            # Value preprocessor type.
#  value_preprocessor_kwargs:     # Value preprocessor kwargs.
#  rewards_shaper:                # Rewards shaper type.

# ========= Model parameters ==========
Model:
  use_init: True
  use_same_model: True                 # If true, use the same model for actor and critic.
  use_action_clip: False               # If true, clip actions to the action space range.
  use_action_out_tanh: False           # If true, use tanh for the output of the actor.
  action_clip: 1.0                     # clipping coefficient for the norm of the actions
  action_scale: 1.0                    # scaling coefficient for the actions
  use_log_std_clip: True               # If true, clip log standard deviations to the range [-20, 2].
  log_std_clip_max: 2.0                # clipping coefficient for the log standard deviations
  log_std_clip_min: -20                # clipping coefficient for the log standard deviations

  actor:
    type: "Gaussian"  # ["Beta", "Gaussian"]
    mlp_hidden_dims: [ 64, 64 ]
#    mlp_hidden_dims: [ 128, 128 ]
    mlp_activation: tanh
    use_lstm: false
    lstm_cell_size: 256
    lstm_use_prev_action: false
    lstm_use_prev_reward: false
    max_seq_len: 20

  critic:
    mlp_hidden_dims: [ 64 ]
    mlp_activation: tanh
    use_lstm: false
    lstm_cell_size: 256
    lstm_use_prev_action: false
    lstm_use_prev_reward: false
    max_seq_len: 20

  discriminator:
    mlp_hidden_dims: [ 1024, 512 ]
    mlp_activation: relu
    use_lstm: false
    lstm_cell_size: 256
    lstm_use_prev_action: false
    lstm_use_prev_reward: false
    max_seq_len: 20